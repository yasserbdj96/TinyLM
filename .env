# ============================================
# FAST TRAINING CONFIGURATION
# ============================================
# Optimized for speed while maintaining quality
# This should train in minutes, not hours!

# MODEL SIZE: SMALL-MEDIUM (~100-150MB)
# Fast training, still good quality
D_MODEL=384
NUM_LAYERS=6
D_FF=1536
NUM_HEADS=8

# VOCABULARY & CONTEXT - OPTIMIZED FOR SPEED
VOCAB_SIZE=8000           # Smaller vocab = faster training
MAX_SEQ_LEN=512           # Shorter context = much faster
DROPOUT=0.1

# TRAINING SETTINGS - SPEED OPTIMIZED
BATCH_SIZE=32             # LARGER batch = faster training
LEARNING_RATE=0.0003      # Higher learning rate = faster convergence
NUM_EPOCHS=3              # Fewer epochs = faster completion
GRADIENT_CLIP=1.0

# DATA SETTINGS - CRITICAL FOR SPEED
TRAIN_DATA_PATH=data/
SEQ_LENGTH=128            # âš¡ MUCH SHORTER = MUCH FASTER (was 512!)

# MODEL SAVE/LOAD
MODEL_SAVE_PATH=models/personal_assistant.pt
CHECKPOINT_DIR=checkpoints/

# GENERATION SETTINGS
TEMPERATURE=0.7
MAX_GENERATION_LENGTH=150
TOP_K=40
TOP_P=0.9

# DEVICE SETTINGS
DEVICE=cuda

# ============================================
# PERFORMANCE NOTES
# ============================================
# With these settings:
# - SEQ_LENGTH=128 instead of 512 = 4x faster!
# - BATCH_SIZE=32 instead of 8 = 4x faster!
# - NUM_LAYERS=6 instead of 12 = 2x faster!
# - NUM_EPOCHS=3 instead of 10 = 3x faster!
# 
# Total speedup: ~96x faster than before!
# Training should take minutes instead of days
#
# If still too slow, reduce SEQ_LENGTH to 64
# ============================================